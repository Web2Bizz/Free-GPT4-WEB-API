services:
  # Nginx reverse proxy
  nginx:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - "15432:15432"
    volumes:
      - "./logs/nginx:/var/log/nginx:rw"
      - "./nginx/nginx.conf:/etc/nginx/nginx.conf:ro"
    depends_on:
      - api
    networks:
      - external
      - internal
    restart: unless-stopped

  # API service with scalable replicas
  api:
    build:
      context: ./llm-api-service
      dockerfile: Dockerfile
    volumes:
      - "./llm-api-service/data:/app/data:rw"
      - "./logs:/app/logs:rw"
    networks:
      - internal
      - external
    dns:
      - 8.8.8.8
      - 8.8.4.4
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENABLE_REQUEST_LOGGING=${ENABLE_REQUEST_LOGGING:-false}
      - PRIVATE_MODE=${PRIVATE_MODE:-false}
      - COOKIE_FILE=${COOKIE_FILE:-/app/data/cookies.json}
      - PROVIDER=${PROVIDER:-You}
      - REMOVE_SOURCES=${REMOVE_SOURCES:-true}
    command: [
      "python", "-m", "freegpt4.FreeGPT4_Server",
      "--log-level", "${LOG_LEVEL:-INFO}",
      "--cookie-file", "${COOKIE_FILE:-/app/data/cookies.json}",
      "--provider", "${PROVIDER:-You}"
    ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5500/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: ${API_REPLICAS:-2}
      resources:
        limits:
          memory: ${API_MEMORY_LIMIT:-512M}
        reservations:
          memory: ${API_MEMORY_RESERVATION:-256M}
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

networks:
  external:
    driver: bridge
  internal:
    driver: bridge
    internal: true